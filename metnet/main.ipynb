{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from compound import Compound\n",
    "from reaction import Reaction\n",
    "from graph import Graph\n",
    "from data import Data\n",
    "\n",
    "# read data from csv\n",
    "cpds = pd.read_csv('../GNN_toxic/data/raw/compounds_final.csv', index_col=0) # containing toxicity\n",
    "rxns = pd.read_csv('data/reactions_final.csv', index_col=0)\n",
    "pairs = pd.read_csv('data/pairs_final.csv', index_col=0)\n",
    "cofactors = pd.read_csv('data/original/cofactors_KEGG.csv')\n",
    "\n",
    "# create class instances\n",
    "data = Data()\n",
    "graph = Graph(pairs=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Compound object for each row in the DataFrame and add it to the data\n",
    "for index, row in cpds.iterrows():\n",
    "    entry = row['Entry']\n",
    "    name = row['Names']\n",
    "    formula = row['Formula']\n",
    "    mw = row['mol_weight']\n",
    "    smiles = row['SMILES']\n",
    "    is_cofactor = row['Entry'] in cofactors['Entry'].values\n",
    "    is_toxic = row['toxic']\n",
    "\n",
    "    compound = Compound(entry, name, formula, mw, smiles, is_cofactor, is_toxic)\n",
    "    data.add_element('compound', compound)\n",
    "\n",
    "# Create a Reaction object for each row in the DataFrame and add it to the data\n",
    "for index, row in rxns.iterrows():\n",
    "    entry = row['Entry']\n",
    "    name = row['Names']\n",
    "    compounds = row['Compound']\n",
    "    enzyme = row['EC Number']\n",
    "\n",
    "    reaction = Reaction(entry, name, compounds, enzyme)\n",
    "    data.add_element('reaction', reaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 8591 \n",
      "# edges: 30081\n",
      "\n",
      "Removing self-loops...\n",
      "# nodes: 8591 \n",
      "# edges: 30026\n"
     ]
    }
   ],
   "source": [
    "# number of times a metabolite apperas on pairs dataset\n",
    "graph.get_number_of_occurences(pairs)\n",
    "graph.create_graph(data=data, pairs=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f589beb6b30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 8591 \n",
      "# edges: 30081\n",
      "\n",
      "Removing self-loops...\n",
      "# nodes: 8591 \n",
      "# edges: 30026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30026/30026 [00:00<00:00, 600875.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct pathway predictions: 14\n",
      "Correct pathway predictions (%): 28.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Graph\n",
    "graph.create_graph(data=data, pairs=pairs)\n",
    "\n",
    "''' \n",
    "*******************************************\n",
    "Validate the methods on validation datasets \n",
    "*******************************************\n",
    "'''\n",
    "######### VALIDATION SET FROM nicepath ###########\n",
    "test_cases = pd.read_csv('data/original/test_cases.csv')\n",
    "test_cases['source'] = test_cases['Pathway '].apply(lambda x: x.split(',')[0])\n",
    "test_cases['target'] = test_cases['Pathway '].apply(lambda x: x.split(',')[len(x.split(','))-1])\n",
    "test_cases['paths_list'] = test_cases['Pathway '].apply(lambda x: x.split(','))\n",
    "\n",
    "paths = graph.simple_weighted_shortest_path(test_cases=test_cases, data=data, method='mol_weight')\n",
    "\n",
    "# ######### NEW VALIDATION SET ###########\n",
    "# pyminer_test = pd.read_csv('data/original/pyminer_validation_set.csv', delimiter=';', header=None, names=['Pathway'])\n",
    "# pyminer_test['source'] = pyminer_test['Pathway'].apply(lambda x: x.split(',')[0])\n",
    "# pyminer_test['target'] = pyminer_test['Pathway'].apply(lambda x: x.split(',')[len(x.split(','))-1])\n",
    "# pyminer_test['paths_list'] = pyminer_test['Pathway'].apply(lambda x: x.split(','))\n",
    "\n",
    "# print('Simple weighted shortes paths:')\n",
    "# paths = graph.simple_weighted_shortest_path(test_cases=pyminer_test, data=data, method='mol_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predicted paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = 'data/results'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "paths.to_csv('data/results/predicted_paths.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph learning and stuff to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: C00082\n",
      "Name: [\"L-Tyrosine\", \"(S)-3-(p-Hydroxyphenyl)alanine\", \"(S)-2-Amino-3-(p-hydroxyphenyl)propionic acid\", \"Tyrosine\"]\n",
      "Formula: C9H11NO3\n",
      "\n",
      "N[C@@H](Cc1ccc(O)cc1)C(=O)O\n",
      "['C00223', 'C12096', 'C00029', 'C00761']\n",
      "['C00029', 'C00761', 'C00223', 'C12096']\n"
     ]
    }
   ],
   "source": [
    "cpd = data.get_compound_by_id('C00082')\n",
    "print(cpd)\n",
    "\n",
    "smile = cpd.smiles\n",
    "print(smile)\n",
    "\n",
    "correct_pathway_example = paths['Pathway'].iloc[1]\n",
    "print(correct_pathway_example)\n",
    "\n",
    "correct_subgraph = graph.G.subgraph(correct_pathway_example)\n",
    "print(correct_subgraph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN! (Maybe good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = graph.G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric\n",
    "import torch\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global mean pooling to obtain graph-level representation\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        \n",
    "        # apply a final classifier\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_G = gg.copy()\n",
    "print(master_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "labels = []\n",
    "\n",
    "for row in range(len(paths)):\n",
    "    sg = master_G.subgraph(paths['Pathway'].iloc[row])\n",
    "    graphs.append(sg)\n",
    "\n",
    "    if paths['Correct'].iloc[row]: label = 1\n",
    "    else: label = 0\n",
    "\n",
    "    labels.append(label)\n",
    "\n",
    "print(\"Graphs:\", graphs)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NetworkX graphs to PyG data\n",
    "data_list = []\n",
    "for graph, label in zip(graphs, labels):\n",
    "    # rename nodes to integers    \n",
    "    mapping = {node: idx for idx, node in enumerate(graph.nodes())}\n",
    "    graph = nx.relabel_nodes(graph, mapping)\n",
    "\n",
    "    # Convert NetworkX graph to PyG data\n",
    "    x = torch.tensor([graph.nodes[node]['mw'] for node in graph.nodes], dtype=torch.float).view(-1, 1)\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    edge_attr = torch.tensor([graph.edges[edge]['mol_weight'] for edge in graph.edges], dtype=torch.float)\n",
    "    y = torch.tensor([label])  # Graph-level label\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    data_list.append(data)\n",
    "\n",
    "# Concatenate all data samples into a single PyG data object\n",
    "data = torch_geometric.data.Batch.from_data_list(data_list)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize GNN model\n",
    "model = GCN(input_dim=data.x.shape[1], hidden_dim=128, output_dim=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in train_data:\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in test_data:\n",
    "        out = model(data)\n",
    "        predicted_labels = out.argmax(dim=1)\n",
    "        true_labels = data.y.view(-1)\n",
    "        correct += (predicted_labels == true_labels).sum().item()\n",
    "        total += len(true_labels)\n",
    "        print(predicted_labels, data.y)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE OF GRAPHNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "# Generate random graphs with labels\n",
    "num_graphs = 5\n",
    "graphs = []\n",
    "labels = []\n",
    "\n",
    "for _ in range(num_graphs):\n",
    "    # Generate a random graph\n",
    "    graph = nx.fast_gnp_random_graph(10, 0.3)\n",
    "    graphs.append(graph)\n",
    "    \n",
    "    # Assign a random label (Type 0 or Type 1)\n",
    "    label = random.choice([0, 1])\n",
    "    labels.append(label)\n",
    "\n",
    "print(\"Graphs:\", graphs)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric\n",
    "import torch\n",
    "\n",
    "# Convert NetworkX graphs to PyG data\n",
    "data_list = []\n",
    "for graph, label in zip(graphs, labels):\n",
    "    # Convert NetworkX graph to PyG data\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    x = torch.randn(graph.number_of_nodes(), 16)  # Random node features (16 dimensions)\n",
    "    y = torch.tensor([label])  # Graph-level label\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    data_list.append(data)\n",
    "\n",
    "\n",
    "# Concatenate all data samples into a single PyG data object\n",
    "data = torch_geometric.data.Batch.from_data_list(data_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global mean pooling to obtain graph-level representation\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize GNN model\n",
    "model = GCN(input_dim=16, hidden_dim=32, output_dim=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in train_data:\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in test_data:\n",
    "        out = model(data)\n",
    "        predicted_labels = out.argmax(dim=1)\n",
    "        true_labels = data.y.view(-1)\n",
    "        correct += (predicted_labels == true_labels).sum().item()\n",
    "        total += len(true_labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
